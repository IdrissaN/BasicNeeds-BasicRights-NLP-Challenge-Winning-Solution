{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Basic Needs Basic Rights Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZaBksOWDodk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8c8a362-1f80-460f-beaf-28cff7a4cbfe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cKNYVsT9_Lmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q simpletransformers\n",
        "!pip install -q gputil\n",
        "!pip install -q psutil\n",
        "!pip install -q humanize"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "cbrOusp-_Lm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import psutil\n",
        "import humanize\n",
        "import os, gc\n",
        "import random, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textblob\n",
        "from nltk import sent_tokenize\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) \n",
        "    np.random.seed(seed_value) \n",
        "    torch.manual_seed(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed = 2\n",
        "seed_all(seed)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IYlsBCG_Lm8",
        "colab_type": "text"
      },
      "source": [
        "## LOAD COMPETITION DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "2KtTG23p_Lm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/'\n",
        "comp = 'Tech4MentalHealth/'\n",
        "translated = 'translated/'\n",
        "\n",
        "TRAIN_DATA_FILE = f'{path}{comp}Train.csv'\n",
        "TEST_DATA_FILE = f'{path}{comp}Test.csv'\n",
        "SAMPLE_SUB_FILE = f'{path}{comp}SampleSubmission.csv'\n",
        "PSEUDO_LABEL = f'{path}pseudolabeldata/data_depression_pseudo_label.csv'\n",
        "\n",
        "train = pd.read_csv(TRAIN_DATA_FILE)\n",
        "test = pd.read_csv(TEST_DATA_FILE)\n",
        "\n",
        "feat_cols = \"text\"\n",
        "labels_ord = ['Alcohol','Depression', 'Drugs', 'Suicide']\n",
        "train['label'] = train.label.astype('category').cat.codes\n",
        "\n",
        "Y = to_categorical(train['label'])\n",
        "\n",
        "for i in range(len(labels_ord)) :     \n",
        "     train[labels_ord[i]] = Y[:,i]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y4DAA17L_LnB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b78346d5-5ec5-4ec3-8b58-1b0d8afd0175"
      },
      "source": [
        "train.head(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Drugs</th>\n",
              "      <th>Suicide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SUAVK39Z</td>\n",
              "      <td>I feel that it was better I dieAm happy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9JDAGUV3</td>\n",
              "      <td>Why do I get hallucinations?</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>419WR1LQ</td>\n",
              "      <td>I am stresseed due to lack of financial suppor...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID                                               text  ...  Drugs  Suicide\n",
              "0  SUAVK39Z            I feel that it was better I dieAm happy  ...    0.0      0.0\n",
              "1  9JDAGUV3                       Why do I get hallucinations?  ...    1.0      0.0\n",
              "2  419WR1LQ  I am stresseed due to lack of financial suppor...  ...    0.0      0.0\n",
              "\n",
              "[3 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOaE51d3_LnE",
        "colab_type": "text"
      },
      "source": [
        "## DATA AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w7zgPu_n_LnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLPTransform(BasicTransform):\n",
        "    \"\"\" Transform for nlp task.\"\"\"\n",
        "    LANGS = {\n",
        "        'en': 'english'\n",
        "    }\n",
        "\n",
        "    @property\n",
        "    def targets(self):\n",
        "        return {\"data\": self.apply}\n",
        "    \n",
        "    def update_params(self, params, **kwargs):\n",
        "        if hasattr(self, \"interpolation\"):\n",
        "            params[\"interpolation\"] = self.interpolation\n",
        "        if hasattr(self, \"fill_value\"):\n",
        "            params[\"fill_value\"] = self.fill_value\n",
        "        return params\n",
        "\n",
        "    def get_sentences(self, text, lang='en'):\n",
        "        return sent_tokenize(text, self.LANGS.get(lang, 'english'))\n",
        "\n",
        "\n",
        "class SwapWordsTransform(NLPTransform):\n",
        "    \"\"\" Swap words next to each other \"\"\"\n",
        "    def __init__(self, swap_distance=1, swap_probability=0.1, always_apply=False, p=0.5):\n",
        "        \"\"\"  \n",
        "        swap_distance - distance for swapping words\n",
        "        swap_probability - probability of swapping for one word\n",
        "        \"\"\"\n",
        "        super(SwapWordsTransform, self).__init__(always_apply, p)\n",
        "        self.swap_distance = swap_distance\n",
        "        self.swap_probability = swap_probability\n",
        "        self.swap_range_list = list(range(1, swap_distance+1))\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        words = text.split()\n",
        "        words_count = len(words)\n",
        "        if words_count <= 1:\n",
        "            return text, lang\n",
        "\n",
        "        new_words = {}\n",
        "        for i in range(words_count):\n",
        "            if random.random() > self.swap_probability:\n",
        "                new_words[i] = words[i]\n",
        "                continue\n",
        "    \n",
        "            if i < self.swap_distance:\n",
        "                new_words[i] = words[i]\n",
        "                continue\n",
        "    \n",
        "            swap_idx = i - random.choice(self.swap_range_list)\n",
        "            new_words[i] = new_words[swap_idx]\n",
        "            new_words[swap_idx] = words[i]\n",
        "\n",
        "        return ' '.join([v for k, v in sorted(new_words.items(), key=lambda x: x[0])]), lang"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qs3kNfyB_LnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CutOutWordsTransform(NLPTransform):\n",
        "    \"\"\" Remove random words \"\"\"\n",
        "    def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):\n",
        "        super(CutOutWordsTransform, self).__init__(always_apply, p)\n",
        "        self.cutout_probability = cutout_probability\n",
        "\n",
        "    def apply(self, data, **params):\n",
        "        text, lang = data\n",
        "        words = text.split()\n",
        "        words_count = len(words)\n",
        "        if words_count <= 1:\n",
        "            return text, lang\n",
        "        \n",
        "        new_words = []\n",
        "        for i in range(words_count):\n",
        "            if random.random() < self.cutout_probability:\n",
        "                continue\n",
        "            new_words.append(words[i])\n",
        "\n",
        "        if len(new_words) == 0:\n",
        "            return words[random.randint(0, words_count-1)], lang\n",
        "\n",
        "        return ' '.join(new_words), lang"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1VgYhJG__LnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "swap_transform = SwapWordsTransform(p=1.0, swap_distance=1, swap_probability=0.2)\n",
        "cutout_transform = CutOutWordsTransform(p=1.0, cutout_probability=0.2)\n",
        "\n",
        "lang = 'en'\n",
        "train_copy = train.copy()\n",
        "\n",
        "for i in range(len(train_copy)):\n",
        "        text = train_copy['text'][i]\n",
        "        text = swap_transform(data=(text, lang))['data'][0]\n",
        "        train_copy['text'][i] = cutout_transform(data=(text, lang))['data'][0]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jnFaZ5Da_LnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "6b320c15-e1b7-4b12-e3f2-c942fd77c375"
      },
      "source": [
        "train_copy.head(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Drugs</th>\n",
              "      <th>Suicide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SUAVK39Z</td>\n",
              "      <td>feel I it that was I better dieAm happy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9JDAGUV3</td>\n",
              "      <td>Why I do hallucinations? get</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>419WR1LQ</td>\n",
              "      <td>I am stresseed due to lack of financial suppor...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6UY7DX6Q</td>\n",
              "      <td>Why is life important?</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FYC0FTFB</td>\n",
              "      <td>How I be to through the depression?</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID                                               text  ...  Drugs  Suicide\n",
              "0  SUAVK39Z            feel I it that was I better dieAm happy  ...    0.0      0.0\n",
              "1  9JDAGUV3                       Why I do hallucinations? get  ...    1.0      0.0\n",
              "2  419WR1LQ  I am stresseed due to lack of financial suppor...  ...    0.0      0.0\n",
              "3  6UY7DX6Q                             Why is life important?  ...    0.0      1.0\n",
              "4  FYC0FTFB                How I be to through the depression?  ...    0.0      0.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyNuAdzRD-DI",
        "colab_type": "text"
      },
      "source": [
        "### ADD AUGMENTED DRUG AND SUICIDE STATEMENTS TO TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bhOiqhxd_LnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_drug_transform = train_copy[train_copy['label']==2]\n",
        "df_suicide_transform = train_copy[train_copy['label']==3]\n",
        "\n",
        "train = pd.concat([train, df_drug_transform, df_suicide_transform], axis=0).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "train_df = train.drop(columns=['ID'])\n",
        "test_df = test.drop(columns=['ID'])\n",
        "\n",
        "for col in labels_ord:\n",
        "    test_df[col] = 0"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF04iCfo_LnU",
        "colab_type": "text"
      },
      "source": [
        "## REMOVE DUPLICATES AND CORRECT MISPELLED WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UGh00dZt_LnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.drop_duplicates(subset =\"text\", keep = False, inplace = True) \n",
        "\n",
        "train_df['text'] = train_df.text.apply(lambda txt: ''.join(textblob.TextBlob(txt).correct()))\n",
        "test_df['text'] = test_df.text.apply(lambda txt: ''.join(textblob.TextBlob(txt).correct()))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBel7Juq_LnX",
        "colab_type": "text"
      },
      "source": [
        "## DEFINE THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TdAMSOVV_LnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(train, model_name, model_config, epochs, max_seq_length, train_batch_size, seed, learning_rate, n_splits):\n",
        "    \n",
        "    loss=[]\n",
        "    y_pred_test=[]\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    \n",
        "    print(f\"Training {n_splits} folds  with --{model_config} --{epochs} epochs\")\n",
        "    \n",
        "    for fold_, (train_index, test_index) in enumerate(skf.split(train, train['label'])):\n",
        "        train1_trn, train1_val = train.iloc[train_index], train.iloc[test_index]\n",
        "        model = ClassificationModel(model_name, model_config, use_cuda=True, num_labels=4, args={\n",
        "                                                                             'train_batch_size':train_batch_size,\n",
        "                                                                             'reprocess_input_data': True,\n",
        "                                                                             'overwrite_output_dir': True,\n",
        "                                                                             'fp16': False,\n",
        "                                                                             'do_lower_case': False,\n",
        "                                                                             'num_train_epochs': epochs,\n",
        "                                                                             'max_seq_length': max_seq_length,\n",
        "                                                                             'regression': False,\n",
        "                                                                             'manual_seed': seed,\n",
        "                                                                             \"learning_rate\":learning_rate,\n",
        "                                                                             'weight_decay':0,\n",
        "                                                                             \"save_eval_checkpoints\": False,\n",
        "                                                                             \"save_model_every_epoch\": False,\n",
        "                                                                             \"silent\": True})\n",
        "        model.train_model(train1_trn)\n",
        "        raw_outputs_val = model.eval_model(train1_val)[1]\n",
        "        raw_outputs_vals = softmax(raw_outputs_val,axis=1)\n",
        "        print(f\"Fold n°{fold_+1} LogLoss : {round(log_loss(train1_val['label'], raw_outputs_vals), 6)}\")\n",
        "        loss.append(log_loss(train1_val['label'], raw_outputs_vals))\n",
        "        raw_outputs_test = model.eval_model(test_df)[1]\n",
        "        raw_outputs_tests = softmax(raw_outputs_test,axis=1)\n",
        "        y_pred_test.append(raw_outputs_tests)\n",
        "        \n",
        "    print(f\"\\nOverall CV LogLoss : {round(np.mean(loss), 6)}\")\n",
        "    \n",
        "    return y_pred_test, loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeIj0gjN_LnZ",
        "colab_type": "text"
      },
      "source": [
        "## Model 1 : RoBERTa base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3bWMll89_Lnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "574da043-5f3a-4aa9-d0cd-08b5d5f55536"
      },
      "source": [
        "preds_roberta_base_1997, loss_roberta_base_1997 = get_model(train_df, 'roberta', 'roberta-base', 4, 128, 16, 1997, 2e-05, 10)\n",
        "\n",
        "sub_model1 = pd.read_csv(SAMPLE_SUB_FILE)\n",
        "preds_model1 = np.mean(preds_roberta_base_1997, 0)\n",
        "\n",
        "for i in range(len(labels_ord)): \n",
        "    sub_model1[labels_ord[i]] = preds_model1[:,i]\n",
        "\n",
        "sub_model1.to_csv('sub_model1_1997.csv',index=False)\n",
        "sub_model1.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 10 folds  with --roberta-base --4 epochs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°1 LogLoss : 0.211707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°2 LogLoss : 0.396107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°3 LogLoss : 0.274473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°4 LogLoss : 0.352644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°5 LogLoss : 0.401113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°6 LogLoss : 0.59562\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°7 LogLoss : 0.504781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°8 LogLoss : 0.313553\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°9 LogLoss : 0.324909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°10 LogLoss : 0.384533\n",
            "\n",
            "Overall CV LogLoss : 0.375944\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Suicide</th>\n",
              "      <th>Drugs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02V56KMO</td>\n",
              "      <td>0.824155</td>\n",
              "      <td>0.021605</td>\n",
              "      <td>0.140665</td>\n",
              "      <td>0.013574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03BMGTOK</td>\n",
              "      <td>0.989920</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>0.005385</td>\n",
              "      <td>0.002088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03LZVFM6</td>\n",
              "      <td>0.989009</td>\n",
              "      <td>0.002708</td>\n",
              "      <td>0.006034</td>\n",
              "      <td>0.002248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0EPULUM5</td>\n",
              "      <td>0.989099</td>\n",
              "      <td>0.002666</td>\n",
              "      <td>0.006220</td>\n",
              "      <td>0.002015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0GM4C5GD</td>\n",
              "      <td>0.013568</td>\n",
              "      <td>0.116576</td>\n",
              "      <td>0.035914</td>\n",
              "      <td>0.833942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID  Depression   Alcohol   Suicide     Drugs\n",
              "0  02V56KMO    0.824155  0.021605  0.140665  0.013574\n",
              "1  03BMGTOK    0.989920  0.002607  0.005385  0.002088\n",
              "2  03LZVFM6    0.989009  0.002708  0.006034  0.002248\n",
              "3  0EPULUM5    0.989099  0.002666  0.006220  0.002015\n",
              "4  0GM4C5GD    0.013568  0.116576  0.035914  0.833942"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aGA23Be_Lnd",
        "colab_type": "text"
      },
      "source": [
        "## Model 2 : RoBERTa base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EVskKiLk_Lnd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05e11958-7631-4654-cbaf-235f335f5212"
      },
      "source": [
        "preds_roberta_base_789, loss_roberta_base_789 = get_model(train_df, 'roberta', 'roberta-base', 4, 128, 16, 789, 2e-05, 10)\n",
        "\n",
        "sub_model2 = pd.read_csv(SAMPLE_SUB_FILE)\n",
        "preds_model2 = np.mean(preds_roberta_base_789, 0)\n",
        "\n",
        "for i in range(len(labels_ord)): \n",
        "    sub_model2[labels_ord[i]] = preds_model2[:,i]\n",
        "\n",
        "sub_model2.to_csv('sub_model2_789.csv',index=False)\n",
        "sub_model2.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 10 folds  with --roberta-base --4 epochs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°1 LogLoss : 0.515488\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°2 LogLoss : 0.317253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°3 LogLoss : 0.264156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°4 LogLoss : 0.350653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°5 LogLoss : 0.461657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°6 LogLoss : 0.321061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°7 LogLoss : 0.265255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°8 LogLoss : 0.354428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°9 LogLoss : 0.562856\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°10 LogLoss : 0.37894\n",
            "\n",
            "Overall CV LogLoss : 0.379175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Suicide</th>\n",
              "      <th>Drugs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02V56KMO</td>\n",
              "      <td>0.869605</td>\n",
              "      <td>0.010437</td>\n",
              "      <td>0.114883</td>\n",
              "      <td>0.005075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03BMGTOK</td>\n",
              "      <td>0.988861</td>\n",
              "      <td>0.003188</td>\n",
              "      <td>0.006091</td>\n",
              "      <td>0.001859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03LZVFM6</td>\n",
              "      <td>0.989934</td>\n",
              "      <td>0.002909</td>\n",
              "      <td>0.005403</td>\n",
              "      <td>0.001754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0EPULUM5</td>\n",
              "      <td>0.988795</td>\n",
              "      <td>0.002959</td>\n",
              "      <td>0.006579</td>\n",
              "      <td>0.001666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0GM4C5GD</td>\n",
              "      <td>0.010971</td>\n",
              "      <td>0.144913</td>\n",
              "      <td>0.021480</td>\n",
              "      <td>0.822636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID  Depression   Alcohol   Suicide     Drugs\n",
              "0  02V56KMO    0.869605  0.010437  0.114883  0.005075\n",
              "1  03BMGTOK    0.988861  0.003188  0.006091  0.001859\n",
              "2  03LZVFM6    0.989934  0.002909  0.005403  0.001754\n",
              "3  0EPULUM5    0.988795  0.002959  0.006579  0.001666\n",
              "4  0GM4C5GD    0.010971  0.144913  0.021480  0.822636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJzSPdhT_Lnf",
        "colab_type": "text"
      },
      "source": [
        "## Model 3 : RoBERTa base 5 folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KZFlg7cy_Lnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "outputId": "4c9ea9ae-142f-45e2-a1e7-91f0fd0b525e"
      },
      "source": [
        "preds_roberta_base_5folds, loss_roberta_base_5folds = get_model(train_df, 'roberta', 'roberta-base', 4, 128, 16, 1997, 2e-05, 5)\n",
        "\n",
        "sub_model3 = pd.read_csv(SAMPLE_SUB_FILE)\n",
        "preds_model3 = np.mean(preds_roberta_base_5folds, 0)\n",
        "\n",
        "for i in range(len(labels_ord)): \n",
        "    sub_model3[labels_ord[i]] = preds_model3[:,i]\n",
        "\n",
        "sub_model3.to_csv('sub_model3_5folds.csv',index=False)\n",
        "sub_model3.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 5 folds  with --roberta-base --4 epochs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°1 LogLoss : 0.329555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°2 LogLoss : 0.356188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°3 LogLoss : 0.422912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°4 LogLoss : 0.439481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fold n°5 LogLoss : 0.336106\n",
            "\n",
            "Overall CV LogLoss : 0.376848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Suicide</th>\n",
              "      <th>Drugs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02V56KMO</td>\n",
              "      <td>0.777048</td>\n",
              "      <td>0.027434</td>\n",
              "      <td>0.168602</td>\n",
              "      <td>0.026915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03BMGTOK</td>\n",
              "      <td>0.986406</td>\n",
              "      <td>0.003262</td>\n",
              "      <td>0.007469</td>\n",
              "      <td>0.002863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03LZVFM6</td>\n",
              "      <td>0.985281</td>\n",
              "      <td>0.003372</td>\n",
              "      <td>0.008298</td>\n",
              "      <td>0.003050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0EPULUM5</td>\n",
              "      <td>0.985636</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.008452</td>\n",
              "      <td>0.002678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0GM4C5GD</td>\n",
              "      <td>0.021768</td>\n",
              "      <td>0.206010</td>\n",
              "      <td>0.051795</td>\n",
              "      <td>0.720427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID  Depression   Alcohol   Suicide     Drugs\n",
              "0  02V56KMO    0.777048  0.027434  0.168602  0.026915\n",
              "1  03BMGTOK    0.986406  0.003262  0.007469  0.002863\n",
              "2  03LZVFM6    0.985281  0.003372  0.008298  0.003050\n",
              "3  0EPULUM5    0.985636  0.003233  0.008452  0.002678\n",
              "4  0GM4C5GD    0.021768  0.206010  0.051795  0.720427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijSOyUog_Lnh",
        "colab_type": "text"
      },
      "source": [
        "## ENSEMBLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hkGSISez_Lni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sub_model1 = pd.read_csv('./sub_model1_1997.csv')\n",
        "# sub_model2 = pd.read_csv('./sub_model2_789.csv')\n",
        "# sub_model3 = pd.read_csv('./sub_model3_5folds.csv')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ONZnY73d_Lnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_1_2 = sub_model1.copy()\n",
        "\n",
        "for col in ensemble_1_2.columns[1:]:\n",
        "    ensemble_1_2[col] = sub_model1[col]*0.68 + sub_model2[col]*0.32\n",
        "\n",
        "ensemble_1_2.to_csv('ensemble_789_1997.csv',index=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhcOSNJo_Lnl",
        "colab_type": "text"
      },
      "source": [
        "## POST-PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RGYkDLGU_Lnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ensemble_1_2_3 = ensemble_1_2.copy()\n",
        "ensemble_1_2_3['Drugs'] = sub_model3['Drugs']*0.8 + ensemble_1_2['Drugs']*0.2\n",
        "ensemble_1_2_3['Suicide'] = sub_model3['Suicide']"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n7UYOALP_Lnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "8912b544-4a10-4297-ccb1-25e52571b4f6"
      },
      "source": [
        "final_sub = ensemble_1_2_3.copy()\n",
        "\n",
        "threshold = 0.025\n",
        "\n",
        "for i,e in enumerate(final_sub['Suicide']):\n",
        "    if e<threshold:\n",
        "        final_sub['Suicide'][i]=0\n",
        "        \n",
        "for i,e in enumerate(final_sub['Drugs']):\n",
        "    if e<threshold:\n",
        "        final_sub['Drugs'][i]=0\n",
        "        \n",
        "for i,e in enumerate(final_sub['Alcohol']):\n",
        "    if e<threshold:\n",
        "        final_sub['Alcohol'][i]=0\n",
        "        \n",
        "for i,e in enumerate(final_sub['Depression']):\n",
        "    if e<threshold:\n",
        "        final_sub['Depression'][i]=0\n",
        "\n",
        "\n",
        "#final_sub.to_csv('final_sub.csv',index=False)\n",
        "\n",
        "final_sub.head(5)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Suicide</th>\n",
              "      <th>Drugs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>02V56KMO</td>\n",
              "      <td>0.838699</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168602</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03BMGTOK</td>\n",
              "      <td>0.989582</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03LZVFM6</td>\n",
              "      <td>0.989305</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0EPULUM5</td>\n",
              "      <td>0.989002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0GM4C5GD</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.125644</td>\n",
              "      <td>0.051795</td>\n",
              "      <td>0.742406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         ID  Depression   Alcohol   Suicide     Drugs\n",
              "0  02V56KMO    0.838699  0.000000  0.168602  0.000000\n",
              "1  03BMGTOK    0.989582  0.000000  0.000000  0.000000\n",
              "2  03LZVFM6    0.989305  0.000000  0.000000  0.000000\n",
              "3  0EPULUM5    0.989002  0.000000  0.000000  0.000000\n",
              "4  0GM4C5GD    0.000000  0.125644  0.051795  0.742406"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjCwNsgOP9R2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "4a64e3f3-087b-4ec5-e9aa-b588910a6797"
      },
      "source": [
        "test_df.head(5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Depression</th>\n",
              "      <th>Drugs</th>\n",
              "      <th>Suicide</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Now to overcome bad feelings and emotions</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I feel like giving up in life</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I was so depressed feel like got no strength t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I feel so low especially since I had no one to...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>can i be successful when I am a drug adduct?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  Alcohol  ...  Drugs  Suicide\n",
              "0          Now to overcome bad feelings and emotions        0  ...      0        0\n",
              "1                     I feel like giving up in life         0  ...      0        0\n",
              "2  I was so depressed feel like got no strength t...        0  ...      0        0\n",
              "3  I feel so low especially since I had no one to...        0  ...      0        0\n",
              "4       can i be successful when I am a drug adduct?        0  ...      0        0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}